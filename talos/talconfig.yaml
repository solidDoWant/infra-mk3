---
clusterName: infra-mk3
endpoint: https://10.3.254.1:6443
nodes:
  - &talos-k8s-mixed
    hostname: talos-k8s-mixed-01
    ipAddress: 10.3.1.1
    installDiskSelector:
      # `/*` suffix is needed until
      # https://github.com/siderolabs/go-blockdevice/issues/114 is fixed
      busPath: /pci0000:00/0000:00:1d.0/0000:59:00.0/nvme/* # M.2 slot 2
    controlPlane: true
    machineSpec:
      secureboot: true
    # Previously, this used separate VLANs for Talos traffic and for
    # Kubernetes traffic. This worked well for traffic within the same subnet,
    # but caused problems with traffic from other subnets. Requests would come
    # in via the Kubernetes VLAN, and responses would be sent out via the
    # management VLAN. Normally this _could_ work, with the right firewall
    # rules, however the major issue was ICMP responses for fragmented packets.
    # With path MTU discovery enabled, ICMP responses would get sent back via
    # the wrong VLAN, and never make their way back to the correct interface.
    # This was likely in part due to how Cilium handles network traffic via
    # eBPF programs attached to specific interfaces.
    nameservers:
      - 10.3.0.254
    networkInterfaces:
      - &1g_1
        interface: enp91s0
        ignore: true
      - &1g_2
        interface: enp94s0
        ignore: true
      - &10g_1
        interface: &10g_1_name enp6s0f0np0
        mtu: &jumbo_mtu 9000
      - &10g_2
        interface: &10g_2_name enp6s0f1np1
        mtu: *jumbo_mtu
      - &bond_0
        interface: bond0
        bond:
          interfaces:
            - *10g_1_name
            - *10g_2_name
          mode: 802.3ad
          lacpRate: fast
          xmitHashPolicy: layer3+4
          miimon: 100
        mtu: *jumbo_mtu
        addresses:
          - 10.3.1.1/16
          - 10.3.2.1/32
        vip:
          ip: 10.3.254.1
        routes:
          - network: 0.0.0.0/0
            gateway: 10.3.0.254
      # Creates a link-scoped route for the loopback interface, fixing Istio compatibility
      # with Cilium eBPF masquerading. This works by sending link-local traffic through
      # iptables, instead of eBPF, which is bugged. Not setting a gateway makes these routes
      # link-scoped.
      # For details, see https://github.com/istio/istio/issues/52208#issuecomment-2488755452
      # This attempts to solve the same issue with talos host DNS caching with the same
      # approach.
      - &loopback
        interface: lo
        routes:
          - network: 169.254.7.127/32
          - network: 169.254.116.108/32
    nodeLabels:
      cilium.home.arpa/node.bgp-enabled: "true"
      root-ceph.flux.home.arpa/node.cluster-enabled: "true"
      katacontainers.io/kata-runtime: "true"
      # Topology labels should contain the label name in the value to ensure
      # that the value itself is unique
      topology.rook.io/chassis: chassis-{{ .MachineConfig.MachineNetwork.NetworkHostname }}
      zfs.home.arpa/node.local-storage-enabled: "true"
      cni-plugins.home.arpa/node.general-purpose-plugins-enabled: "true"
    nodeAnnotations:
      zfs.home.arpa/node.pool-drive-matcher: "/dev/disk/by-id/nvme-KINGSTON_SKC3000D2048G_*"
      talos.home.arpa/installer-image: >-
        {{
          .MachineConfig.MachineInstall.InstallImage |
          splitList ":" |
          first
        }}
    schematic:
      customization: &schematic_customization
        systemExtensions:
          officialExtensions:
            # Needed or the node will not boot
            - siderolabs/i915
            - siderolabs/intel-ucode
            - siderolabs/mei
            - siderolabs/thunderbolt
            # For virtualization/isolation
            - siderolabs/kata-containers
            # Local storage
            - siderolabs/zfs
        # TODO set the following args on the installer only (not ISO):
        # src: https://cdrdv2-public.intel.com/332464/332464_710_Series_Datasheet_v_4_1.pdf
        # - intel_iommu=on
        # - iommu=pt
        extraKernelArgs:
          # Make ZFS flush to disk more frequently on async writes:
          # https://vadosware.io/post/everything-ive-seen-on-optimizing-postgres-on-zfs-on-linux/#controversial-zfs_txg_timeout1--synchronous_commitoff--logbiasthroughput
          - zfs.zfs_txg_timeout=1
          # Disable ipv6 via command line parameter
          # This one actually works
          - ipv6.disable=1
          # Log forwarding to the node agent
          - talos.logging.kernel=tcp://127.0.0.1:5171/
    imageSchematic:
      customization:
        <<: *schematic_customization
        extraKernelArgs:
          - &bond bond=bond0:enp2s0f0np0,enp2s0f1np1:mode=802.3ad,xmit_hash_policy=layer3+4:9000
          - ip=10.3.1.1::10.3.0.254:255.255.0.0:talos-k8s-mixed-01:bond0:off:10.3.0.254::10.3.0.254
          - &halt_if_installed -talos.halt_if_installed # Negate this option to allow the installer to run. Needed for recovery
    kernelModules:
      - name: zfs
  - <<: *talos-k8s-mixed
    hostname: talos-k8s-mixed-02
    ipAddress: 10.3.1.2
    networkInterfaces:
      - *1g_1
      - *1g_2
      - *10g_1
      - *10g_2
      - <<: *bond_0
        addresses:
          - 10.3.1.2/16
          - 10.3.2.2/32
      - *loopback
    imageSchematic:
      customization:
        <<: *schematic_customization
        extraKernelArgs:
          - *bond
          - ip=10.3.1.2::10.3.0.254:255.255.0.0:talos-k8s-mixed-02:bond0:off:10.3.0.254::10.3.0.254
          - *halt_if_installed
  - <<: *talos-k8s-mixed
    hostname: talos-k8s-mixed-03
    ipAddress: 10.3.1.3
    networkInterfaces:
      - *1g_1
      - *1g_2
      - *10g_1
      - *10g_2
      - <<: *bond_0
        addresses:
          - 10.3.1.3/16
          - 10.3.2.3/32
      - *loopback
    imageSchematic:
      customization:
        <<: *schematic_customization
        extraKernelArgs:
          - *bond
          - ip=10.3.1.3::10.3.0.254:255.255.0.0:talos-k8s-mixed-03:bond0:off:10.3.0.254::10.3.0.254
          - *halt_if_installed
talosVersion: v1.10.5
kubernetesVersion: v1.33.2
allowSchedulingOnMasters: true
allowSchedulingOnControlPlanes: true
additionalApiServerCertSans: &sans
  - 127.0.0.1 # KubePrism
  # TODO add kube-vip address
additionalMachineCertSans: *sans
# Don't deploy a CNI by default
cniConfig:
  name: none
clusterPodNets:
  - 10.32.0.0/16
clusterSvcNets:
  - 10.33.0.0/16
patches:
  # Don't include the kernel args for non-ISO boot. Otherwise the image is
  # invalidated and secure boot won't load it.
  - |-
    - op: remove
      path: /machine/install/extraKernelArgs
  # Use NTP server running on the gateway
  - |-
    machine:
      time:
        servers:
          - 10.3.0.254
  # Root FS disk encryption
  # TODO: implement disk encryption. Use a HA KMS server implementation
  # for the first key, hosted in the cluster. If that isn't available
  # (such as the case when all nodes are offline), require a HSM to decrypt
  # the first disk.
  # - |-
  #   machine:
  #     systemDiskEncryption:
  #       state:
  #         provider: luks2
  #         keys:
  #           - slot: 0
  #             tpm:
  #               checkSecurebootStatusOnEnroll: true
  #           - slot: 1
  #             static:
  #               passphrase: ${luks_password}
  # Enable Talos API access from k8s pods
  # Needed for OS upgrades
  - |-
    machine:
      features:
        kubernetesTalosAPIAccess:
          enabled: true # Enable Talos API access from Kubernetes pods.
          allowedRoles:
            - os:reader
            - os:admin
          allowedKubernetesNamespaces:
            - system-controllers
  # Enable KubePrism
  - |-
    machine:
      features:
        kubePrism:
          enabled: true
          port: 7443  # 6443 (API server default) + 1000
  # Configure DNS resolution
  - |-
    machine:
      features:
        hostDNS:
          enabled: true
          # This is broken with Cilium, see 
          # * https://github.com/cilium/cilium/issues/35153
          # * https://github.com/siderolabs/talos/pull/9200
          forwardKubeDNSToHost: false
          resolveMemberNames: true
  # Configure kubelet
  - |-
    machine:
      kubelet:
        extraConfig:
          # Equivalent of `rotate-server-certificates` arg, but config instead
          serverTLSBootstrap: true
          imageMaximumGCAge: 168h # One week
          imageGCHighThresholdPercent: 50
          imageGCLowThresholdPercent: 20
          maxPods: 128  # Probably higher than will realistically be scheduled
          serializeImagePulls: false
          allowedUnsafeSysctls:
            # These are needed for the VPN router pods
            # This is scoped wider than I'd like but it's needed until
            # https://github.com/kubernetes/kubernetes/issues/134003 is addressed
            - net.ipv4.conf.*
            - net.ipv4.fib_multipath_hash_seed
            - net.ipv4.fib_multipath_hash_policy
            - net.ipv4.vs.*
            # I'm not using these yet, but they might be useful in future
            # - net.ipv4.fib_multipath_hash_fields
            # - net.ipv4.ip_forward
        extraArgs:
          # Feature enablement
          feature-gates: DynamicResourceAllocation=true,DRAAdminAccess=true,UserNamespacesPodSecurityStandards=true
        nodeIP:
          validSubnets:
            - 10.3.1.0/16
  # Set user namespace sysctls
  # See https://www.talos.dev/v1.9/kubernetes-guides/configuration/usernamespace/#enabling-usernamespaces
  - |-
    machine:
      sysctls:
        user.max_user_namespaces: "11255"
  # Configure containerd
  - |-
    machine:
      files:
        - op: create
          path: /etc/cri/conf.d/20-customization.part
          permissions: 0o644
          content: |
            [plugins]
              # Spegel setup
              [plugins.'io.containerd.cri.v1.images']
                discard_unpacked_layers = false
              # Enable the Container Device Interface
              # Important: this matches https://www.talos.dev/v1.10/talos-guides/configuration/containerd/#set-cdi-plugin-spec-dirs-to-writable-directories,
              # which differs https://github.com/cncf-tags/container-device-interface/blob/fc17f514320237420424663de905720e11b1668b/README.md#containerd-configuration
              [plugins."io.containerd.cri.v1.runtime"]
                enable_cdi = true
                cdi_spec_dirs = ["/var/cdi/static", "/var/cdi/dynamic"]
  # Configure containerd metrics
  # Disabled for now, this destroyed my cluster somehow last time it was enabled
  # - |-
  #   machine:
  #     files:
  #       - op: create
  #         path: /etc/cri/conf.d/30-metrics.part
  #         permissions: 0o644
  #         content: |
  #           [metrics]
  #             address = '0.0.0.0:11234'
  # Configure etcd
  # TODO limit this to processes on the each. Will probably require a custom extension, see
  # https://github.com/siderolabs/talos/blob/3e16ab135e2be8c9b652d67f9e7eadbc3691c5ca/internal/app/machined/pkg/controllers/network/nftables_chain_config.go#L162
  # Or maybe just a new address resource + nftableschain resource
  - |-
    cluster:
      etcd:
        advertisedSubnets:
          - 10.3.1.0/16
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
  # Configure the controller manager
  - |-
    cluster:
      controllerManager:
        extraArgs:
          bind-address: 0.0.0.0
          # Feature enablement
          feature-gates: kube:DynamicResourceAllocation=true,kube:DRAAdminAccess=true,kube:DRAPartitionableDevices=true
  # Configure the scheduler
  - |-
    cluster:
      scheduler:
        extraArgs:
         bind-address: 0.0.0.0
  # Configuration for Cilium
  - |-
    cluster:
      # Use Cilium's kube-proxy replacement
      proxy:
        disabled: true
  # Disable Talos-installed coredns, and instead install it manually
  - |-
    cluster:
      coreDNS:
        disabled: true
  # Enable only local etcd-based discovery
  - |-
    cluster:
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: false
          service:
            disabled: true
  # Enable workload scheduling on the control plane nodes
  - |-
    cluster:
      allowSchedulingOnControlPlanes: true
  # API server configuration
  - |-
    cluster:
      apiServer:
        extraArgs:
          # Load balance requests to metrics-server and other API server
          # services/extensions by sending traffic to each endpoint instead of the
          # service cluster IP
          enable-aggregator-routing: "true"
          # Feature enablement
          # TODO remove AuthorizeNodeWithSelectors=false after Cilium adds load-balanced, HA
          # kube-apiserver support (close: https://github.com/cilium/cilium/issues/29323).
          # Sidero Labs pulled the rug out from under customers and just suddenly stopped
          # Talos etcd-backed "Cluster Discovery" support, which is required for kubeprism.
          # Disabling this flag allows kubeprism to continue to work.
          # See https://github.com/siderolabs/talos/issues/9980
          feature-gates: kube:DynamicResourceAllocation=true,kube:DRAAdminAccess=true,kube:DRAPartitionableDevices=true,kube:AuthorizeNodeWithSelectors=false,kube:UserNamespacesPodSecurityStandards=true
          # Resource enablement
          # Enable everything but alpha and legacy features
          runtime-config: "api/all=true,api/legacy=false"
  # Scheduler configuration
  - |-
    cluster:
      scheduler:
        # Enable cluster-level topology spread constraints for supported controllers
        config:
          profiles:
            - schedulerName: default-scheduler
              pluginConfig:
                - name: PodTopologySpread
                  args:
                    defaultConstraints:
                      - maxSkew: 1 # Skew of 1 allows for rolling updates
                        topologyKey: kubernetes.io/hostname
                        whenUnsatisfiable: DoNotSchedule
                    defaultingType: List
        extraArgs:
          # Feature enablement
          feature-gates: kube:DynamicResourceAllocation=true,kube:DRAAdminAccess=true,kube:DRAPartitionableDevices=true
  # Ship logs to each node's agent
  - |-
    machine:
      logging:
        destinations:
          - endpoint: tcp://127.0.0.1:5170
            format: json_lines

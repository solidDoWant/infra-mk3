---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app_name rook-ceph-cluster
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
      version: v1.16.2
  values:
    operatorNamespace: storage
    clusterName: *app_name
    configOverride: |
      [global]
      # Allow the pool to operate when one node is offline
      osd_pool_default_min_size = 2
      # Enable TRIM support, see https://github.com/rook/rook/issues/6964
      bdev_enable_discard = true
      bdev_async_discard = true
      # Quit trying to mark nvme drives as SSDs upon OSD pod startup
      osd_class_update_on_start = false
      # Configuration for disk predictor module
      device_failure_prediction_mode = local
    toolbox:
      enabled: true
      resources:
        requests:
          memory: 256Mi
        limits:
          memory: 256Mi
    monitoring:
      enabled: true
      createPrometheusRules: true
    pspEnable: false
    cephClusterSpec:
      mgr:
        modules:
          - name: rook
            enabled: true
          - name: diskprediction_local
            enabled: true
          - name: insights
            enabled: true
          - name: rgw
            enabled: true
      dashboard:
        # SSO cannot currently be enabled. Due to a Ceph bug, SSO requires HTTPS
        # even when the dashboard is behind a reverse proxy. This means that the
        # reverse proxy must communicate with the dashboard over HTTPS. When
        # using the Gateway API, this requires the use of a BackendTLSPolicy.
        # The gateway API implementation that I'm currently using (Istio) does
        # not support this. Therefore, the dashboard cannot have SSO enabled.
        # encryption in transit will be handled by istio
        # Related links:
        # * Ceph HTTPS bug: https://tracker.ceph.com/issues/48306, https://github.com/rook/rook/issues/8633
        # * Istio lack of support for BackendTLSPolicy: https://github.com/istio/istio/issues/50408
        # * Dashboard SSO setup docs: https://docs.ceph.com/en/quincy/mgr/dashboard/#dashboard-sso-support
        # To enable SSO (after the above is resolved):
        # 1. Deploy the authentik SAML provider and application. ACS URL is
        #    https://rook.${SECRET_PUBLIC_DOMAIN_NAME}/auth/saml2
        # 2. Setup SSO via the toolbox pod:
        #    `ceph dashboard sso setup saml2 https://rook.${SECRET_PUBLIC_DOMAIN_NAME} https://authentik.${SECRET_PUBLIC_DOMAIN_NAME}/api/v3/providers/saml/3/metadata/?download
        # 3. Enable SSO:
        #    `ceph dashboard sso enable saml2`
        ssl: false
        # TODO prom deployment
        # prometheusEndpoint:
      network:
        connection:
          encryption:
            # TODO consider "offloading" this to istio
            enabled: true
          compression:
            enabled: true
      placement:
        all:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: root-ceph.flux.home.arpa/node.cluster-enabled
                      operator: In
                      values:
                        - "true"
          # TODO pod affinity to schedule different components on the same nodes
          # podAffinity:
          # TODO TSC per component to force them to separate nodes
          # topologySpreadConstraints:
      # TODO measure actual usage and set this
      # resources:
      storage:
        useAllNodes: true
        useAllDevices: false
        # TODO talos 1.9.0 will replace eudev with systemd-udev, which includes
        # udev rules for `/disk/by-path`. Use this instead once supported.
        # Alternatively, if https://github.com/rook/rook/issues/15010 is
        # implemented, use node annotations instead.
        devicePathFilter: /dev/disk/by-id/nvme-SAMSUNG_MZQL21T9HCJR-.* # cspell:disable-line
      csi:
        # Prefer reading from the same node
        # If more Ceph nodes are ever added to another "domain" (such as a separate switch),
        # then the label list will need to be updated
        readAffinity:
          enabled: true
          crushLocationLabels:
            - topology.rook.io/chassis
    cephBlockPools:
      - name: ssd-replicated-3x
        spec:
          failureDomain: chassis
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ssd-replicated-3x
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            compression_mode: aggressive
            # Docs specifically say that this performs poorly, but there are no recent benchmarks, so I'm trying it anyway.
            compression_algorithm: zstd
            # abort_on_full: if the cluster or device is full, fail writes.
            # ms_mode=secure: require encrypted network connections
            mapOptions: abort_on_full,ms_mode=secure
            imageFormat: "2"
            imageFeatures: layering,exclusive-lock,object-map,fast-diff,deep-flatten
            # Default values for the chart
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: &secret_namespace storage
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: *secret_namespace
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: *secret_namespace
            csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: ssd-replicated-3x-block
      isDefault: true
    cephFileSystems:
      - name: ssd-replicated
        spec:
          metadataPool:
            name: ssd-replicated-filesystem-metadata
            failureDomain: chassis
            replicated:
              size: 3
          dataPools:
            - name: ssd-replicated-filesystem-data0
              failureDomain: chassis
              replicated:
                size: 3
          metadataServer:
            # Active/passive
            activeCount: 1
            activeStandby: true
            resources:
              limits:
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 1Gi
            placement:
              topologySpreadConstraints:
                - maxSkew: 1
                  topologyKey: kubernetes.io/hostname
                  labelSelector:
                    matchLabels:
                      # cspell:words cephfilesystems
                      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
                      app.kubernetes.io/name: ceph-mds
                      app.kubernetes.io/part-of: ssd-replicated
                  whenUnsatisfiable: DoNotSchedule
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          name: ssd-replicated-filesystem
          pool: ssd-replicated-filesystem-data0
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
          parameters:
            # Default values for the chart
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: &secret_namespace storage
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: *secret_namespace
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: *secret_namespace
            csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: ssd-replicated-filesystem
      isDefault: true
    cephObjectStores:
      - name: ssd-replicated
        spec:
          metadataPool:
            failureDomain: chassis
            replicated:
              size: 3
          dataPool:
            failureDomain: chassis
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          allowUsersInNamespaces:
            - "*"
          gateway:
            port: 80
            # Required until https://github.com/rook/rook/issues/15048 is fixed
            securePort: 443
            sslCertificateRef: s3-public-domain-name-cert
            resources:
              limits:
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 1Gi
            instances: 2
            placement:
              topologySpreadConstraints:
                - maxSkew: 1
                  topologyKey: kubernetes.io/hostname
                  labelSelector:
                    matchLabels:
                      # cspell:words cephobjectstores
                      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
                      app.kubernetes.io/name: ceph-rgw
                      app.kubernetes.io/instance: ssd-replicated
                  whenUnsatisfiable: DoNotSchedule
            priorityClassName: system-cluster-critical
          hosting:
            advertiseEndpoint:
              dnsName: s3.${SECRET_PUBLIC_DOMAIN_NAME}
              port: 443
              useTls: true
            dnsNames:
              - s3.${SECRET_PUBLIC_DOMAIN_NAME}
        storageClass:
          enabled: true
          name: ssd-replicated-object
          reclaimPolicy: Delete
          volumeBindingMode: Immediate

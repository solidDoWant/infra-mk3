---
# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-k8s-stack
spec:
  interval: 5m
  chart:
    spec:
      chart: victoria-metrics-k8s-stack
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: victoria-metrics-charts
      version: 0.38.0
  values:
    global:
      cluster:
        dnsDomain: cluster.local
      license:
        keyRef:
          name: victoria-metrics-license-key
          key: licenseKey
    victoria-metrics-operator:
      fullnameOverride: victoria-metrics-operator
      crds:
        enabled: false
        plain: false
      operator:
        # Delete the VM version of prom resources if the prom resources are deleted
        enable_converter_ownership: true
      replicaCount: 2
      podDisruptionBudget:
        enabled: true
        minAvailable: 1
      topologySpreadConstraints:
        - maxSkew: 1 # Skew of 1 allows for rolling updates
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: victoria-metrics-operator
          whenUnsatisfiable: DoNotSchedule
      admissionWebhooks:
        certManager:
          enabled: true
          issuer:
            name: monitoring-intermediary-ca
            kind: Issuer
            group: cert-manager.io
          cert:
            # cert-manager ca injector will handle updating the webhook
            # Kyverno will reload the actual deployment
            duration: 4h
    defaultDashboards:
      grafanaOperator: &grafana_operator
        enabled: true
        spec:
          instanceSelector:
            matchLabels:
              dashboards: null
              grafana.home.arpa/instance: grafana
    external:
      grafana:
        # This is passed to alert rules for links to the Grafana dashboard
        host: grafana.${SECRET_PUBLIC_DOMAIN_NAME}
    vmsingle:
      enabled: false
    vmcluster:
      enabled: true
      spec:
        # This can be large because downsampling is reducing the amount of data stored.
        # If this cluster actually lasts for more than a few years and I'm running out of space,
        # I'll either change the downsampling period or add more storage (which will be cheaper).
        retentionPeriod: 10y
        replicationFactor: &replication_factor 2
        # Non-root user, limited disk write access, drop capabilities, etc.
        useStrictSecurity: true
        # This is an application-aware load balancer which should better
        # determine the best backend to send traffic to than Cilium
        requestsLoadBalancer:
          enabled: true
          spec: &ha_component
            image:
              # This needs to be specified manually until https://github.com/VictoriaMetrics/helm-charts/issues/2023 is fixed
              tag: v1.112.0-enterprise
            hostAliases: &host_aliases
              - ip: ${VICTORIA_METRICS_LICENSE_SERVICE_IP}
                hostnames:
                  - license.victoriametrics.com
            podDisruptionBudget:
              minAvailable: 1
            replicaCount: 2
            useStrictSecurity: true
            # If these components fail then there will be no notification that something is going wrong
            priorityClassName: system-cluster-critical
            topologySpreadConstraints:
              - maxSkew: 1 # Skew of 1 allows for rolling updates
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vmclusterlb-vmauth-balancer
                whenUnsatisfiable: DoNotSchedule
            volumes:
              - name: victoria-metrics-license-service-serving-cert
                secret:
                  secretName: victoria-metrics-license-service-serving-cert
                  defaultMode: 0440
                  items:
                    - key: tls.crt
                      path: tls.crt
            volumeMounts:
              - name: victoria-metrics-license-service-serving-cert
                mountPath: /etc/ssl/certs/tls.crt
                subPath: tls.crt
        vmselect:
          <<: *ha_component
          image:
            # This needs to be specified manually until https://github.com/VictoriaMetrics/helm-charts/issues/2023 is fixed
            tag: v1.112.0-enterprise-cluster
          # TODO this will take som tuning and requires historic metrics to get right
          # Probably isn't needed at all given that two replicas min will be running
          # hpa:
          #   minReplicas: 2
          #   maxReplicas: 5
          # Cache storage. Not critical. Docs suggest a small (single-digit GB) amount.
          # Because it's small and needs to be fast, I'm putting it on a memory-backed
          # temporary volume.
          storage:
            emptyDir:
              # Important for resource sizing: the limit is counted gainst the pod's
              # memory limit.
              medium: Memory
              sizeLimit: 2Gi
          topologySpreadConstraints:
            - maxSkew: 1 # Skew of 1 allows for rolling updates
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmselect
              whenUnsatisfiable: DoNotSchedule
        vminsert:
          <<: *ha_component
          resources:
            requests:
              memory: 1Gi
              cpu: 250m
            limits:
              memory: 1Gi
              cpu: 1500m
          image:
            # This needs to be specified manually until https://github.com/VictoriaMetrics/helm-charts/issues/2023 is fixed
            tag: v1.112.0-enterprise-cluster
          topologySpreadConstraints:
            - maxSkew: 1 # Skew of 1 allows for rolling updates
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vminsert
              whenUnsatisfiable: DoNotSchedule
          # This is outrageously high but I'd like to get the data in first so that I can
          # more easily tell which labels I can drop
          extraArgs:
            maxLabelsPerTimeseries: "150"
        vmstorage:
          <<: *ha_component
          resources:
            requests:
              memory: 4Gi
              cpu: 1000m
            limits:
              memory: 4Gi
              cpu: 2000m
          image:
            # This needs to be specified manually until https://github.com/VictoriaMetrics/helm-charts/issues/2023 is fixed
            tag: v1.112.0-enterprise-cluster
          # This must be set to at least three for high availability. Data must always
          # be accessible, even when at most one replica is unavailable. This means that
          # new data must be recorded at least twice, otherwise, when the only replica
          # with a given piece of data is unavailable, that data is also unavailable.
          # With only two replicas and a replicationFactor of 2, this means that when
          # one replica is unavailable, current information is still available, but
          # new information will only be recorded once, making the once-recorded data
          # unavailable when the unavailable and available replicas are switched.
          # This can only be reduced to two when:
          # * VictoriaMetrics adds support for rebalancing data automatically (manual currently planned, automatic not)
          #   * https://github.com/VictoriaMetrics/VictoriaMetrics/issues/188
          #   * https://github.com/VictoriaMetrics/VictoriaMetrics/issues/2324
          # * vminsert instances (not vmagent) support caching data and later pushing
          #   it when a storage replica is temporarily unavailable
          replicaCount: 3 # 2 * replicationFactor - 1
          podDisruptionBudget:
            minAvailable: *replication_factor
          storage:
            volumeClaimTemplate:
              spec:
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 10Gi # This is probably way low, but I need to see rate of growth to properly estimate it.
                # TODO this needs tuning and benchmarking.
                storageClassName: victoria-metrics-vmstorage
                volumeMode: Filesystem
          extraArgs:
            downsampling.period: 30d:1m,180d:30m,1y:1h,2y:4h,5y:12h,10y:1d
            # retentionFilter: # TODO set retention filters for specific low value, high volume metrics
          topologySpreadConstraints:
            - maxSkew: 1 # Skew of 1 allows for rolling updates
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmstorage
              whenUnsatisfiable: DoNotSchedule
          # vmBackup: # TODO maybe. Metrics are not critical to back up.
    alertmanager:
      spec:
        ############# ha_component #############
        # The ha_component anchor currently requires image.tag to be set, but it shouldn't be here
        # Remove this block once the linked issue is fixed
        hostAliases:
          - ip: ${VICTORIA_METRICS_LICENSE_SERVICE_IP}
            hostnames:
              - license.victoriametrics.com
        podDisruptionBudget:
          minAvailable: 1
        replicaCount: 2
        useStrictSecurity: true
        # If these components fail then there will be no notification that something is going wrong
        priorityClassName: system-cluster-critical
        ############# ha_component #############
        topologySpreadConstraints:
          - maxSkew: 1 # Skew of 1 allows for rolling updates
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmalertmanager
            whenUnsatisfiable: DoNotSchedule
        externalURL: https://alertmanager.${SECRET_PUBLIC_DOMAIN_NAME}
        # Upstream alertmanager storage requirements and usage is really poorly documented.
        # Based on https://groups.google.com/g/prometheus-developers/c/KQ5UbAbaYnU, it looks
        # like storage is used for recording:
        # * Silences
        # * Notification states
        # These are propagated throughout the cluster via the gossip protocol.
        # Because performance is not critical, but availability is, I'm putting this on
        # a replicated storage backend. Using an emptyDir volume may be sufficient, if
        # restarted instances receive the state via the gossip protocol before actually
        # processing alerts. However, if the entire cluster is rebooted at once (such as
        # when power is lost and UPS runs out of energy), the state will be lost.
        storage:
          volumeClaimTemplate:
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi # Probably way larger than needed, but still pretty small.
              storageClassName: ssd-replicated-3x
              volumeMode: Filesystem
      # config: # TODO
      # TODO discord receiver(s). Send different priority and/or labeled alerts to different channels, with different notification settings.
    # vmagent and vmalert instances point at the wrong service, but there isn't an easy workaround for this.
    # They should point at the vmclusterlb instance instead of vmselect and vminsert directly. For details, see
    # https://github.com/VictoriaMetrics/helm-charts/issues/2024
    vmalert:
      # Must be set when using multiple vminsert instances (which is the case when using a cluster)
      remoteWriteVMAgent: true
      spec:
        <<: *ha_component
        extraArgs:
          external.url: https://vm-alerts.${SECRET_PUBLIC_DOMAIN_NAME}
        topologySpreadConstraints:
          - maxSkew: 1 # Skew of 1 allows for rolling updates
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmalert
            whenUnsatisfiable: DoNotSchedule
    vmagent:
      # TODO check each instance to verify that they're all scraping the same targets
      spec:
        <<: *ha_component
        resources:
          requests:
            memory: 2Gi
            cpu: 1000m
          limits:
            memory: 2Gi
            cpu: 2000m
        extraArgs:
          # This is because cilium is exporting a ton of metrics, largely due to some pods making IPv6 requests which are unsupported on my network.
          # TODO find the pods and fix them
          promscrape.maxScrapeSize: 32MiB
        topologySpreadConstraints:
          - maxSkew: 1 # Skew of 1 allows for rolling updates
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmagent
            whenUnsatisfiable: DoNotSchedule
    defaultDatasources:
      grafanaOperator: *grafana_operator
    grafana:
      enabled: false
      forceDeployDatasource: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      replicas: 2
      podDisruptionBudget:
        minAvailable: 1
      topologySpreadConstraints:
        - maxSkew: 1 # Skew of 1 allows for rolling updates
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: kube-state-metrics
          whenUnsatisfiable: DoNotSchedule
      vmScrape:
        spec:
          # This is a special case because multiple replicas (should)
          # report the same metrics. Telling VM to scrape the service
          # rather than the underlying pods prevents duplicate metrics.
          discoveryRole: service
    prometheus-node-exporter:
      fullnameOverride: prometheus-node-exporter
    kubeControllerManager:
      service:
        selector:
          k8s-app: kube-controller-manager
          component: ~
    kubelet:
      vmScrape:
        spec:
          relabelConfigs:
            # This is the same as the chart default, but it explicitly excludes NFD node labels,
            # and talos extension labels. These don't really provide any value, and they cause
            # cardinality to explode.
            - action: labelmap
              regex: __meta_kubernetes_node_label_(?!feature_node|extensions_talos_dev)(.+)
            - sourceLabels: [__metrics_path__]
              targetLabel: metrics_path
            - targetLabel: job
              replacement: kubelet
    coreDns:
      # This is handled by the coredns deployments
      enabled: false
    kubeEtcd:
      service:
        selector:
          # This is a workaround for targeting etcd instance on all control planes.
          # Because etcd does not run in a pod, it cannot be directly targeted via a service.
          # Instead, the service must select pods that run on the same nodes as the etcd instances,
          # and also have host networking enabled. The controller manager and scheduler are good
          # choices because they meet these requirements.
          k8s-app: kube-controller-manager
          component: ~
        # This port does not require authentication, and only serves metrics. TODO build automation
        # to pull in the etcd certs via talosctl, which can be called via pods with a Talos service account.
        # Then the normal, authenticated 2379 port can be used. Or even better, move the root CA out
        # of the cluster (probably into a HSM), and issue these certs with it.
        targetPort: 2381
      vmScrape:
        spec:
          endpoints:
            - port: http-metrics
              scheme: http
    kubeScheduler:
      service:
        selector:
          k8s-app: kube-scheduler
          component: ~
  postRenderers:
    # Fix the webhook certificate to meet reasonable standards
    # The added fields will not be needed upon next release
    - kustomize:
        patches:
          - patch: |
              # Set the:
              # * Subject
              # * Secret labels
              # * Common name
              - op: add
                path: /spec/subject
                value:
                  countries:
                    - US
                  provinces:
                    - ${SECRET_STATE}
                  organizations:
                    - infra-mk3
              - op: add
                path: /spec/secretTemplate
                value:
                  labels:
                    kyverno.home.arpa/reload: "true"
              - op: add
                path: /spec/commonName
                value: VictoriaMetrics operator
              # Remove the:
              # * DNS name without the svc or svc.cluster.local domains
              - op: remove
                path: /spec/dnsNames/0
              # Because this is the second remove operation, the old second item is now the first
              - op: remove
                path: /spec/dnsNames/0
            target:
              group: cert-manager.io
              version: v1
              kind: Certificate
              name: victoria-metrics-operator-validation

# Motivation and background
Over the past 10 to 15 years I've gone through several iterations of my home infrastructure, including both hardware and software. I have made mistakes every time, however, I can confidently say that each major iteration has grown closer to an ill-defined "ideal" design. More importantly though, I have gained significant knowledge (primarily technical) by building out and maintaining this infrastructure for years. Not only do I enjoy learning and improving my skill set, but this hobby has drastically helped my career development as well.

Here's a high-level overview of some of the major "revisions" of my infrastructure:

* Originally I manually deployed Windows Server VMs on my desktop with VMWare Workstations. Early on in my career, I worked in a small business environment, managing Windows desktops, Cisco Meraki-based networks, along with a couple of CentOS VMs running on a desktop "server" that lived in a closet somewhere. The knowledge I gained while deploying Windows Servers, Active Directory domains, and other assorted services helped the company I worked for deploy backups, setup single sign-on, and build out networks that (to my knowledge) are still in use today. This mitigated risk, improved security, and improved productivity.

* On the second iteration, I ran Linux, Windows, and BSD VMs via VMWare ESXi. This all ran on a couple of seventh-gen HPE DL385s that a coworker was kind enough to give me during a company test environment hardware refresh. These dual processor, 16 core (total) machines with 24 GB of 1333 MHz DDR3 kept my homelab running for a couple of years. I ran a OPNSense VM on one as my first non-consumer router, as well as some video game servers for friends. I also ran some Ubuntu VMs for Linux kernel development of an [AT91SAM9XE512](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/1048/AT91SAM7X512%2C256%2C128.pdf)-based rocket flight computer for a non-profit company that I co-founded. This infrastructure, and the projects I used it for, helped take me from a Linux user to a proper admin. I later put these skills to use for my employer, where I (amongst other projects) re-architectured and migrated the hosting environment for a major e-commerce platform. This resulted in faster, more secure deployments, as  well as higher scalability and reliability. If you live in the US, there is a decent chance that you've used this platform at some point.

* [The most recent iteration of my homelab infrastructure](https://github.com/soliddowant/infra-mk2) is a pretty major divergence from the last two. When I started out with this version, I decided that I wanted it to be as easy to maintain and reliable as possible. [At a hardware level](https://github.com/solidDoWant/infra-mk2/tree/main/hardware), I (mostly) accomplished this with an assortment of enterprise servers, a NetApp disk shelf, and a Brocade 40 Gbps access switch. However, the biggest change wasn't the hardware - it was the software. Rather than running VMs, almost all of my workloads run as containers in a Kubernetes cluster. Almost all of the hardware was provisioned with Ansible, and all persistent data could tolerate at least two disk failures without data loss. Most of the workloads could automatically shift between server, so I could perform maintenance (manually or via automation) with minimal interruption. Nearly all changes have been recorded via git in the linked repo, and were automatically deployed. This made it easy to tell what changed and when, as well as roll back if necessary. 

  As a result, I ran at least ten times as much software on this stack compared to previous iterations, but with a significantly lower maintenance burden. This allowed me to learn about, deploy, and test out lots of different pieces of software that in many cases I ended up deploying in a production environment. I later scratch-built a set of similar clusters for my employer. With this pattern, I was able to manage business-critical services with nearly zero downtime, almost singlehandedly. On top of this, the lessons I've learned from my homelab reduced my employer's (directly measurable) recurring costs by nearly a $1MM/year _last year alone_. This is a non-trivial portion of their annual R&D budget and has quantifiably extended the company's runway.

With my experience with the last three iterations in mind, there are several things that I would like to improve on with this iteration:

* I need (even) higher reliability. I have several workloads that require specific hardware (such as a GPU) to operate. Currently, these workloads can only run on one node. This means when this node fails (which unfortunately is happening with increasing frequency), the workloads also fail.

  Additionally, my OPNSense (router) instance is a single point of failure for basically everything. Updates are always risky because if the instance fails to start, I usually have to recover it from the locally, as my network is effectively torched. I need a second standby instance with automatic failover for when this happens.

  I don't expect to get rid of all internal single points of failure entirely - however, I want to reduce them as much as is feasibly and financially possible.

* I want to improve my data resiliency. Currently all my backups are manual and recovery is often untested. I need to setup automated backups of important data, and I need to be able to store a copy of the most critical data off-site in a location that I control (as in, not a SaaS vendor).

* I need to reduce the power consumption of my hardware. My server rack currently pulls about a kilowatt on average, which ends up enormously expensive. Even if it comes at an increased up-front cost, I need to reduce my rack's power usage.
